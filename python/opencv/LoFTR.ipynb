{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e89b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoImageProcessor, AutoModelForKeypointMatching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611f27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(\"zju-community/efficientloftr\")\n",
    "model = AutoModelForKeypointMatching.from_pretrained(\"zju-community/efficientloftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415afdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(image1_path, image2_path, score_threshold=0.95):\n",
    "    # --- Create output directory ---\n",
    "    os.makedirs(\"LoFTR_results\", exist_ok=True)\n",
    "    filename = os.path.basename(image1_path).split('.')[0] + \"_\" + os.path.basename(image2_path).split('.')[0]\n",
    "\n",
    "    # --- Load images ---\n",
    "    image1 = Image.open(image1_path).convert(\"RGB\")\n",
    "    image2 = Image.open(image2_path).convert(\"RGB\")\n",
    "    images = [image1, image2]\n",
    "    \n",
    "    # --- Preprocess and run model ---\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Format sizes as expected: List of [(H, W), (H, W)]\n",
    "    image_sizes = [(image.height, image.width) for image in images]\n",
    "    processed_outputs = processor.post_process_keypoint_matching(outputs, [image_sizes], threshold=score_threshold)\n",
    "\n",
    "    # --- Convert PIL to NumPy ---\n",
    "    img1 = np.array(image1)\n",
    "    img2 = np.array(image2)\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "\n",
    "    # --- Create side-by-side canvas ---\n",
    "    gap = 20\n",
    "    out_img = np.zeros((max(h1, h2), w1 + w2 + gap, 3), dtype=np.uint8)\n",
    "    out_img[:h1, :w1] = img1\n",
    "    out_img[:h2, w1 + gap:] = img2\n",
    "\n",
    "    # --- Draw matches ---\n",
    "    match_count = 0\n",
    "    for kp0, kp1, score in zip(\n",
    "        processed_outputs[0][\"keypoints0\"],\n",
    "        processed_outputs[0][\"keypoints1\"],\n",
    "        processed_outputs[0][\"matching_scores\"]\n",
    "    ):\n",
    "        if score < score_threshold:\n",
    "            continue\n",
    "        pt1 = tuple(np.round(kp0.numpy()).astype(int))\n",
    "        pt2 = tuple(np.round(kp1.numpy()).astype(int) + np.array([w1 + gap, 0]))\n",
    "        color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "        cv2.line(out_img, pt1, pt2, color, 2, cv2.LINE_AA)\n",
    "        cv2.circle(out_img, pt1, 2, color, -1, cv2.LINE_AA)\n",
    "        cv2.circle(out_img, pt2, 2, color, -1, cv2.LINE_AA)\n",
    "        match_count += 1\n",
    "\n",
    "    # --- Save image ---\n",
    "    save_path = os.path.join(\"LoFTR_results\", filename + \".png\")\n",
    "    cv2.imwrite(save_path, out_img[:, :, ::-1])\n",
    "    print(f\"[✓] Saved {match_count} high-confidence matches at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd64c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Saved 201 high-confidence matches at: LoFTR_results/3373_3374.png\n",
      "[✓] Saved 203 high-confidence matches at: LoFTR_results/3374_3375.png\n",
      "[✓] Saved 185 high-confidence matches at: LoFTR_results/3375_3376.png\n",
      "[✓] Saved 168 high-confidence matches at: LoFTR_results/3376_3377.png\n",
      "[✓] Saved 180 high-confidence matches at: LoFTR_results/3377_3381.png\n",
      "[✓] Saved 231 high-confidence matches at: LoFTR_results/3381_3382.png\n",
      "[✓] Saved 266 high-confidence matches at: LoFTR_results/3382_3383.png\n",
      "[✓] Saved 206 high-confidence matches at: LoFTR_results/3383_3384.png\n",
      "[✓] Saved 233 high-confidence matches at: LoFTR_results/3384_3385.png\n",
      "[✓] Saved 218 high-confidence matches at: LoFTR_results/3385_3389.png\n",
      "[✓] Saved 303 high-confidence matches at: LoFTR_results/3389_3390.png\n",
      "[✓] Saved 314 high-confidence matches at: LoFTR_results/3390_3391.png\n",
      "[✓] Saved 325 high-confidence matches at: LoFTR_results/3391_3392.png\n",
      "[✓] Saved 357 high-confidence matches at: LoFTR_results/3392_3393.png\n",
      "[✓] Saved 6 high-confidence matches at: LoFTR_results/3393_3397.png\n",
      "[✓] Saved 636 high-confidence matches at: LoFTR_results/3397_3398.png\n",
      "[✓] Saved 585 high-confidence matches at: LoFTR_results/3398_3399.png\n",
      "[✓] Saved 159 high-confidence matches at: LoFTR_results/3399_3400.png\n",
      "[✓] Saved 4 high-confidence matches at: LoFTR_results/3400_3401.png\n",
      "[✓] Saved 15 high-confidence matches at: LoFTR_results/3401_3405.png\n",
      "[✓] Saved 62 high-confidence matches at: LoFTR_results/3405_3406.png\n",
      "[✓] Saved 65 high-confidence matches at: LoFTR_results/3406_3407.png\n",
      "[✓] Saved 69 high-confidence matches at: LoFTR_results/3407_3408.png\n",
      "[✓] Saved 132 high-confidence matches at: LoFTR_results/3408_3409.png\n",
      "[✓] Saved 4 high-confidence matches at: LoFTR_results/3409_3413.png\n",
      "[✓] Saved 435 high-confidence matches at: LoFTR_results/3413_3414.png\n",
      "[✓] Saved 406 high-confidence matches at: LoFTR_results/3414_3415.png\n",
      "[✓] Saved 437 high-confidence matches at: LoFTR_results/3415_3416.png\n",
      "[✓] Saved 599 high-confidence matches at: LoFTR_results/3416_3417.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Path to directory ---\n",
    "image_dir = \"LoFTR_images\"\n",
    "\n",
    "# --- List and sort image filenames ---\n",
    "image_files = sorted([\n",
    "    f for f in os.listdir(image_dir)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "])\n",
    "\n",
    "# --- Loop through consecutive pairs ---\n",
    "for i in range(len(image_files) - 1):\n",
    "    img1_path = os.path.join(image_dir, image_files[i])\n",
    "    img2_path = os.path.join(image_dir, image_files[i + 1])\n",
    "    find_matches(img1_path, img2_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
